{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NDimVQVAE Docs PyTorch Implementation of the N-Dimensional VQ-VAE by AdityaNG Install it from PyPI pip install nd_vq_vae Cite Cite our work if you find it useful! @article{NG2024D3Nav, title={D\u00b3Nav: Data-Driven Driving Agents for Autonomous Vehicles in Unstructured Traffic}, author={Aditya NG and Gowri Srinivas}, journal={The 35th British Machine Vision Conference (BMVC)}, year={2024}, url={https://bmvc2024.org/} } Development Read the CONTRIBUTING.md file.","title":"NDimVQVAE Docs"},{"location":"#ndimvqvae-docs","text":"PyTorch Implementation of the N-Dimensional VQ-VAE by AdityaNG","title":"NDimVQVAE Docs"},{"location":"#install-it-from-pypi","text":"pip install nd_vq_vae","title":"Install it from PyPI"},{"location":"#cite","text":"Cite our work if you find it useful! @article{NG2024D3Nav, title={D\u00b3Nav: Data-Driven Driving Agents for Autonomous Vehicles in Unstructured Traffic}, author={Aditya NG and Gowri Srinivas}, journal={The 35th British Machine Vision Conference (BMVC)}, year={2024}, url={https://bmvc2024.org/} }","title":"Cite"},{"location":"#development","text":"Read the CONTRIBUTING.md file.","title":"Development"},{"location":"examples/","text":"Usage How to use NDimVQVAE Below is an example of encoding temporal video data. Video data is 3D since it spans height and width as well as time. Note that the channels are each treated separately and does not count as a dimension. from nd_vq_vae import NDimVQVAE sequence_length = 3 channels = 3 res = (128, 256) input_shape = (channels, sequence_length, res[0], res[1]) model = NDimVQVAE( embedding_dim=64, n_codes=64, n_dims=3, downsample=args.downsample, n_hiddens=64, n_res_layers=2, codebook_beta=0.10, input_shape=input_shape, ) x = torch.randn(batch_size, *input_shape) recon_loss, x_recon, vq_output = model(x) 3D: Train on Videos Videos are 3 dimensional data with (Time, Height, Width). You can construct a video dataset at data/video_dataset/ as follows: $ tree data/video_dataset/ data/video_dataset/ \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 Gu1D3BnIYZg.mkv # you can add more videos to both folders \u2514\u2500\u2500 train \u2514\u2500\u2500 ceEE_oYuzS4.mp4 2D: Train on Images Videos are 2 dimensional data with (Height, Width). You can construct a video dataset at data/image_dataset/ as follows: $ tree data/image_dataset/ data/image_dataset/ \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 0000001.png # you can add more images to both folders \u2514\u2500\u2500 train \u2514\u2500\u2500 0000001.png Then you can use the video training script: python scripts/train_image.py --data_path data/image_dataset/ 1D Coming soon!","title":"Usage"},{"location":"examples/#usage","text":"","title":"Usage"},{"location":"examples/#how-to-use-ndimvqvae","text":"Below is an example of encoding temporal video data. Video data is 3D since it spans height and width as well as time. Note that the channels are each treated separately and does not count as a dimension. from nd_vq_vae import NDimVQVAE sequence_length = 3 channels = 3 res = (128, 256) input_shape = (channels, sequence_length, res[0], res[1]) model = NDimVQVAE( embedding_dim=64, n_codes=64, n_dims=3, downsample=args.downsample, n_hiddens=64, n_res_layers=2, codebook_beta=0.10, input_shape=input_shape, ) x = torch.randn(batch_size, *input_shape) recon_loss, x_recon, vq_output = model(x)","title":"How to use NDimVQVAE"},{"location":"examples/#3d-train-on-videos","text":"Videos are 3 dimensional data with (Time, Height, Width). You can construct a video dataset at data/video_dataset/ as follows: $ tree data/video_dataset/ data/video_dataset/ \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 Gu1D3BnIYZg.mkv # you can add more videos to both folders \u2514\u2500\u2500 train \u2514\u2500\u2500 ceEE_oYuzS4.mp4","title":"3D: Train on Videos"},{"location":"examples/#2d-train-on-images","text":"Videos are 2 dimensional data with (Height, Width). You can construct a video dataset at data/image_dataset/ as follows: $ tree data/image_dataset/ data/image_dataset/ \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 0000001.png # you can add more images to both folders \u2514\u2500\u2500 train \u2514\u2500\u2500 0000001.png Then you can use the video training script: python scripts/train_image.py --data_path data/image_dataset/","title":"2D: Train on Images"},{"location":"examples/#1d","text":"Coming soon!","title":"1D"},{"location":"hyperparameter_tuning/","text":"Hyperparameters: How to tune my VQ-VAE? Hyperparameters Overview The VQ-VAE has several key hyperparameters: Codebook size and embedding dimension (n_codes and embedding_dim), model capacity (n_hiddens and n_res_layers), downsampling strategy (downsample), loss balancing (codebook_beta and recon_loss_factor), optimization parameters (learning_rate, beta1, beta2), training parameters (batch_size, num_epochs), attention mechanism (n_head, attn_dropout), and codebook update strategy (ema_decay). Tuning Process Monitor Key Metrics During training and validation, it's crucial to track reconstruction loss, commitment loss, and perplexity. Note that the reconstruction loss are in the same units as the input data. Say the input data is in meters, that means that the reconstruction loss will also be in meters! Analyze Loss Curves Reconstruction Loss If the reconstruction loss is high and not decreasing, consider increasing model capacity (n_hiddens, n_res_layers) or adjusting the learning rate. When the training loss is decreasing but validation loss remains stable, it may indicate potential overfitting. In this case, reduce capacity or add regularization. If both training and validation losses are decreasing, but validation loss is much higher, try increasing batch_size or using data augmentation. Commitment Loss For high commitment loss, decrease codebook_beta. If it's too low or unstable, increase codebook_beta. Balance Losses Adjust codebook_beta and recon_loss_factor to achieve a good balance between reconstruction and commitment losses. Optimize Codebook Usage Monitor perplexity closely. Low perplexity suggests increasing n_codes or decreasing embedding_dim, while high perplexity indicates the need to decrease n_codes or increase embedding_dim. Fine-tune Learning Dynamics For slow convergence, increase learning_rate or adjust optimizer parameters. If training is unstable, decrease learning_rate or increase batch_size. Address Overfitting When validation loss plateaus while training loss decreases, consider introducing dropout in encoder/decoder, reducing model capacity, or increasing batch_size / using data augmentation. Attention Mechanism Adjust n_head and attn_dropout in attention blocks to improve long-range dependencies. Codebook Update Strategy Fine-tune ema_decay for optimal codebook stability and adaptation speed. Downsampling Strategy Adjust downsample factors based on available computational resources and required detail level. Best Practices Make incremental changes to hyperparameters and perform ablation studies, changing one parameter at a time. Consider using learning rate scheduling or cyclical learning rates. Regularly save checkpoints and log experiments for comparison.","title":"Hyperparameters: How to tune my VQ-VAE?"},{"location":"hyperparameter_tuning/#hyperparameters-how-to-tune-my-vq-vae","text":"","title":"Hyperparameters: How to tune my VQ-VAE?"},{"location":"hyperparameter_tuning/#hyperparameters-overview","text":"The VQ-VAE has several key hyperparameters: Codebook size and embedding dimension (n_codes and embedding_dim), model capacity (n_hiddens and n_res_layers), downsampling strategy (downsample), loss balancing (codebook_beta and recon_loss_factor), optimization parameters (learning_rate, beta1, beta2), training parameters (batch_size, num_epochs), attention mechanism (n_head, attn_dropout), and codebook update strategy (ema_decay).","title":"Hyperparameters Overview"},{"location":"hyperparameter_tuning/#tuning-process","text":"","title":"Tuning Process"},{"location":"hyperparameter_tuning/#monitor-key-metrics","text":"During training and validation, it's crucial to track reconstruction loss, commitment loss, and perplexity. Note that the reconstruction loss are in the same units as the input data. Say the input data is in meters, that means that the reconstruction loss will also be in meters!","title":"Monitor Key Metrics"},{"location":"hyperparameter_tuning/#analyze-loss-curves","text":"","title":"Analyze Loss Curves"},{"location":"hyperparameter_tuning/#reconstruction-loss","text":"If the reconstruction loss is high and not decreasing, consider increasing model capacity (n_hiddens, n_res_layers) or adjusting the learning rate. When the training loss is decreasing but validation loss remains stable, it may indicate potential overfitting. In this case, reduce capacity or add regularization. If both training and validation losses are decreasing, but validation loss is much higher, try increasing batch_size or using data augmentation.","title":"Reconstruction Loss"},{"location":"hyperparameter_tuning/#commitment-loss","text":"For high commitment loss, decrease codebook_beta. If it's too low or unstable, increase codebook_beta.","title":"Commitment Loss"},{"location":"hyperparameter_tuning/#balance-losses","text":"Adjust codebook_beta and recon_loss_factor to achieve a good balance between reconstruction and commitment losses.","title":"Balance Losses"},{"location":"hyperparameter_tuning/#optimize-codebook-usage","text":"Monitor perplexity closely. Low perplexity suggests increasing n_codes or decreasing embedding_dim, while high perplexity indicates the need to decrease n_codes or increase embedding_dim.","title":"Optimize Codebook Usage"},{"location":"hyperparameter_tuning/#fine-tune-learning-dynamics","text":"For slow convergence, increase learning_rate or adjust optimizer parameters. If training is unstable, decrease learning_rate or increase batch_size.","title":"Fine-tune Learning Dynamics"},{"location":"hyperparameter_tuning/#address-overfitting","text":"When validation loss plateaus while training loss decreases, consider introducing dropout in encoder/decoder, reducing model capacity, or increasing batch_size / using data augmentation.","title":"Address Overfitting"},{"location":"hyperparameter_tuning/#attention-mechanism","text":"Adjust n_head and attn_dropout in attention blocks to improve long-range dependencies.","title":"Attention Mechanism"},{"location":"hyperparameter_tuning/#codebook-update-strategy","text":"Fine-tune ema_decay for optimal codebook stability and adaptation speed.","title":"Codebook Update Strategy"},{"location":"hyperparameter_tuning/#downsampling-strategy","text":"Adjust downsample factors based on available computational resources and required detail level.","title":"Downsampling Strategy"},{"location":"hyperparameter_tuning/#best-practices","text":"Make incremental changes to hyperparameters and perform ablation studies, changing one parameter at a time. Consider using learning rate scheduling or cyclical learning rates. Regularly save checkpoints and log experiments for comparison.","title":"Best Practices"},{"location":"install/","text":"Install PyPI pip install nd_vq_vae Development Environment Clone the repo git clone https://github.com/AdityaNG/nD_VQ_VAE cd nD_VQ_VAE/ Setup the environment make virtualenv source .venv/bin/activate make install Before making a commit, make sure to run the linting, formatting and testing make fmt make lint make test","title":"Install"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#pypi","text":"pip install nd_vq_vae","title":"PyPI"},{"location":"install/#development-environment","text":"Clone the repo git clone https://github.com/AdityaNG/nD_VQ_VAE cd nD_VQ_VAE/ Setup the environment make virtualenv source .venv/bin/activate make install Before making a commit, make sure to run the linting, formatting and testing make fmt make lint make test","title":"Development Environment"}]}